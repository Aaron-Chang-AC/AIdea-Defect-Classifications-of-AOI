{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3Yw-I6QsVZx","outputId":"2036dae3-6ba3-4aba-8cce-ca901fb991ef","executionInfo":{"status":"ok","timestamp":1668578708542,"user_tz":-480,"elapsed":28324,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"pjVX0fQH-LQO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668578726772,"user_tz":-480,"elapsed":14769,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}},"outputId":"ba5ebc72-e1a2-4bdf-9cb4-23aa9b94adbe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 32.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 68.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 61.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","source":["import os\n","\n","os.chdir(\"drive/MyDrive/Colab Notebooks/AOI\")"],"metadata":{"id":"WXVkLRgosd4T","executionInfo":{"status":"ok","timestamp":1668578726772,"user_tz":-480,"elapsed":13,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","from tqdm import tqdm\n","from transformers import get_scheduler\n","from transformers import AutoFeatureExtractor, ViTMAEModel, ViTModel\n","\n","# the class for training data\n","class AOI_Dataset(Dataset):\n","    def __init__(self, root_dir, annotation_file, feature_extractor=None, transform1=None, transform2=None):\n","        self.root_dir = root_dir\n","        self.annotations = pd.read_csv(annotation_file)\n","\n","        # feature extraction\n","        self.transform1 = transform1\n","        self.transform2 = transform2\n","        self.feature_extractor = feature_extractor\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_id = self.annotations.iloc[index, 0]\n","        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","        y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n","        img1 = self.transform1(img)\n","        img1 = self.feature_extractor(img1)\n","        img2 = self.transform2(img)\n","        img2 = self.feature_extractor(img2)\n","        return (img1, img2 , y_label)"],"metadata":{"id":"fkE4ISHHsjuH","executionInfo":{"status":"ok","timestamp":1668579349010,"user_tz":-480,"elapsed":667,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# the structure of the model\n","class AOI_network(nn.Module):\n","  def __init__(self, num_classes=6):\n","    super(AOI_network, self).__init__()\n","    self.ViT1 = ViTModel.from_pretrained('facebook/vit-mae-base')\n","    self.ViT2 = ViTModel.from_pretrained('facebook/vit-mae-base')\n","    self.out_fc = nn.Linear(self.ViT1.config.hidden_size * 2, num_classes)\n","    self.dropout = nn.Dropout(0.1)\n","\n","  def forward(self, img1, img2):\n","    features1 = self.ViT1(img1).pooler_output\n","    features2 = self.ViT2(img2).pooler_output\n","    features_t = torch.cat([features1, features2], axis=1)\n","    out = self.dropout(self.out_fc(features_t))\n","    return out"],"metadata":{"id":"BE3-XO4sthz4","executionInfo":{"status":"ok","timestamp":1668579353071,"user_tz":-480,"elapsed":643,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# transformations can be specified here\n","# the goal of the second transformation is sharpening the images as the second\n","# input images for the model\n","transform_01 = transforms.Compose(\n","        [\n","            transforms.Resize((224, 224)),\n","            transforms.RandomRotation((10)),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","transform_02 = transforms.Compose(\n","        [\n","            transforms.RandomAdjustSharpness(4, p=1),\n","            transforms.Resize((224, 224)),\n","            transforms.RandomRotation((10)),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","num_epochs = 5\n","learning_rate = 2e-4\n","batch_size = 16\n","shuffle = True\n","num_workers = 1"],"metadata":{"id":"m_vD4k73xkFD","executionInfo":{"status":"ok","timestamp":1668579358091,"user_tz":-480,"elapsed":4,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# all the training images must be in the directory \"train_images\"\n","model_name_or_path = 'facebook/vit-mae-base'\n","feature_extractor_01 = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n","dataset = AOI_Dataset(\"train_images\",\"train.csv\", feature_extractor=feature_extractor_01, transform1=transform_01, transform2=transform_02)\n","train_loader = DataLoader(dataset=dataset, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers)\n","\n","model = AOI_network().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","learning_rate_scheduler = get_scheduler(\n","        \"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_epochs * len(train_loader),\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0eU8MsQo1bz-","outputId":"26f317fe-aa66-431c-85c6-c5d2b2699bf8","executionInfo":{"status":"ok","timestamp":1668579381771,"user_tz":-480,"elapsed":7165,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.mask_token', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.bias']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.mask_token', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.bias']\n","- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# load model if needed\n","checkpoint = torch.load(\"checkpoint\", map_location=device)\n","model.load_state_dict(checkpoint['state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","epoch = checkpoint['epoch']"],"metadata":{"id":"_SWR3_zVjIa7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSmw9k9ijp3f","outputId":"af8cb012-c1ba-4f42-f180-155c75032cdd","executionInfo":{"status":"ok","timestamp":1668579395627,"user_tz":-480,"elapsed":704,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    initial_lr: 0.0002\n","    lr: 0.0002\n","    maximize: False\n","    weight_decay: 0.01\n",")\n"]}]},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","  model.train()\n","  loop = tqdm(train_loader, total = len(train_loader), leave = True)\n","  for imgs1, imgs2, labels in loop:\n","    imgs1 = imgs1['pixel_values'][0].type(torch.FloatTensor).to(device)\n","    imgs2 = imgs2['pixel_values'][0].type(torch.FloatTensor).to(device)\n","    labels = labels.type(torch.LongTensor).to(device)\n","    optimizer.zero_grad()\n","    outputs = model(imgs1, imgs2)\n","    # print(outputs.shape, labels.shape)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    learning_rate_scheduler.step()\n","    loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","    loop.set_postfix(loss = loss.item())\n","  checkpoint = {\n","                'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                'optimizer': optimizer.state_dict()\n","  }\n","  torch.save(checkpoint, \"checkpoint\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhlE8aM32faq","outputId":"db93d2e7-99d0-4d40-eb5a-eb58f18a48ad","executionInfo":{"status":"ok","timestamp":1668582291736,"user_tz":-480,"elapsed":2895436,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch [1/5]: 100%|██████████| 158/158 [34:40<00:00, 13.17s/it, loss=0.457]\n","Epoch [2/5]: 100%|██████████| 158/158 [02:57<00:00,  1.12s/it, loss=0.0517]\n","Epoch [3/5]: 100%|██████████| 158/158 [02:56<00:00,  1.12s/it, loss=0.00667]\n","Epoch [4/5]: 100%|██████████| 158/158 [02:56<00:00,  1.12s/it, loss=0.0169]\n","Epoch [5/5]: 100%|██████████| 158/158 [02:56<00:00,  1.12s/it, loss=0.0485]\n"]}]},{"cell_type":"code","source":["# an extra class for testing data\n","class AOI_Dataset_TEST(Dataset):\n","    def __init__(self, root_dir, annotation_file, feature_extractor=None, transform1=None, transform2=None):\n","        self.root_dir = root_dir\n","        self.annotations = pd.read_csv(annotation_file)\n","\n","        # feature extraction\n","        self.transform1 = transform1\n","        self.transform2 = transform2\n","        self.feature_extractor = feature_extractor\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_id = self.annotations.iloc[index, 0]\n","        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","        img1 = self.transform1(img)\n","        img1 = self.feature_extractor(img1)\n","        img2 = self.transform2(img)\n","        img2 = self.feature_extractor(img2)\n","\n","        return (img1, img2)"],"metadata":{"id":"ml1elbT5DQRV","executionInfo":{"status":"ok","timestamp":1668582291737,"user_tz":-480,"elapsed":8,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","model.eval()\n","# all the testing images must be in the directory \"test_images\"\n","transform_01 = transforms.Compose(\n","        [\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","transform_02 = transforms.Compose(\n","        [\n","            transforms.RandomAdjustSharpness(4, p=1),\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","dataset_TEST = AOI_Dataset_TEST(\"test_images\", \"test.csv\", feature_extractor=feature_extractor_01, transform1=transform_01, transform2=transform_02)\n","test_loader = DataLoader(dataset=dataset_TEST, shuffle=False, batch_size=batch_size,num_workers=num_workers)\n","\n","loop = tqdm(test_loader, total = len(test_loader), leave = True)\n","pred_labels = []\n","res = []\n","\n","for imgs1, imgs2 in loop:\n","  imgs1 = imgs1['pixel_values'][0].type(torch.FloatTensor).to(device)\n","  imgs2 = imgs2['pixel_values'][0].type(torch.FloatTensor).to(device)\n","  outputs = model(imgs1, imgs2)\n","\n","  pred = torch.argmax(torch.softmax(outputs, dim=1), dim=-1).to(device, dtype=torch.int8)\n","  pred_labels.append(pred.cpu().detach().numpy().tolist())\n","\n","# the original csv file \"test.csv\" containing the names of all images\n","pred_labels = np.asarray(pred_labels)\n","for i in pred_labels:\n","  for j in i:\n","    res.append(j)\n","res = np.array(res)\n","out_df = pd.read_csv(\"test.csv\")\n","print(res)\n","print(len(out_df))\n","out_df[\"Label\"] = res\n","\n","# the new output file can be specified here\n","out_df.to_csv(\"test_output.csv\", encoding='utf-8', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lo7N3UyJC1il","outputId":"e8c70b7a-216d-4c63-e272-22a2649abd87","executionInfo":{"status":"ok","timestamp":1668588368115,"user_tz":-480,"elapsed":6076384,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 634/634 [1:41:14<00:00,  9.58s/it]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["[1 2 5 ... 1 3 1]\n","10142\n"]}]}]}